-----------------------------------------------------------------------
code to find the accuracy of bacterial image classification using ANN
-----------------------------------------------------------------------
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
from sklearn.model_selection import train_test_split

# Load data
data = np.load('bacterial_data.npy')
labels = np.load('bacterial_labels.npy')

# Split data into training and testing sets
train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)

# Create the model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=train_data.shape[1:]))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(train_data, train_labels, epochs=10)

# Evaluate the model on test data
test_loss, test_acc = model.evaluate(test_data, test_labels)

print('Test accuracy:', test_acc)
----------------------------------------------------------------------------
preprocessing of image dataset mounted from drive
-------------------------------------------------------------------------------
import os
import numpy as np
from PIL import Image
from google.colab import drive

# Mount Google Drive
drive.mount('/content/gdrive')

# Set data path
data_path = '/content/gdrive/My Drive/dataset/'

# Get list of image files
img_files = [f for f in os.listdir(data_path) if f.endswith('.jpg')]

# Set image size
img_size = (224, 224)

# Initialize NumPy array for images and labels
X = np.zeros((len(img_files), img_size[0], img_size[1], 3))
y = np.zeros(len(img_files))

# Loop over images and labels
for i, img_file in enumerate(img_files):
    # Load image
    img = Image.open(os.path.join(data_path, img_file))
    
    # Resize image
    img = img.resize(img_size)
    
    # Convert image to NumPy array
    img_arr = np.array(img)
    
    # Add image to X array
    X[i] = img_arr
    
    # Extract label from filename
    label = int(img_file.split('_')[0])
    
    # Add label to y array
    y[i] = label
    
# Normalize pixel values
X = X / 255.0

# Shuffle data
perm = np.random.permutation(len(X))
X = X[perm]
y = y[perm]
-------------------------------------------------------------------------------------------------------
code for preprocessing for background correction,image resizing,image enhancement and color space transformation
------------------------------------------------------------------------------------------------------
import cv2
import numpy as np

# Load image
img = cv2.imread('image.jpg')

# Convert to grayscale
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Apply background correction
bg = cv2.medianBlur(gray, 21)
diff = cv2.absdiff(gray, bg)
thresh = cv2.threshold(diff, 10, 255, cv2.THRESH_BINARY)[1]

# Resize image
resized = cv2.resize(thresh, (224, 224))

# Apply image enhancement
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
enhanced = clahe.apply(resized)

# Convert to LAB color space
lab = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
lab = cv2.cvtColor(lab, cv2.COLOR_BGR2LAB)

# Split channels and normalize
l, a, b = cv2.split(lab)
l = cv2.normalize(l, None, 0, 255, cv2.NORM_MINMAX)
a = cv2.normalize(a, None, 0, 255, cv2.NORM_MINMAX)
b = cv2.normalize(b, None, 0, 255, cv2.NORM_MINMAX)

# Merge channels
lab_norm = cv2.merge((l, a, b))

# Convert back to BGR color space
result = cv2.cvtColor(lab_norm, cv2.COLOR_LAB2BGR)
------------------------------------------------------------------------------
code for finding accuracy in ANN using image dataset
----------------------------------------------------------------------
import tensorflow as tf
import numpy as np

# Load the image dataset and corresponding labels
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

# Preprocess the data by scaling the pixel values to between 0 and 1
X_train = X_train / 255.0
X_test = X_test / 255.0

# Reshape the image data into 1D arrays
X_train = np.reshape(X_train, (X_train.shape[0], 784))
X_test = np.reshape(X_test, (X_test.shape[0], 784))

# Define the ANN model
model = tf.keras.Sequential([
  tf.keras.layers.Dense(128, activation='relu', input_dim=784),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model on the training data
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test data and print the accuracy
test_loss, test_acc = model.evaluate(X_test, y_test)
print('Test accuracy:', test_acc)
-------------------------------------------------------------------
# apply the same preprocessing steps to each image
for filename in filenames:
    # load the image
    image = plt.imread(os.path.join('/content/drive/MyDrive/ANN', filename))

    # apply multiple preprocessing steps
    gray_image = color.rgb2gray(image)
    denoised_image = filters.median(gray_image)
    resized_image = transform.resize(denoised_image, (256, 256))

    # display the preprocessed image
    plt.imshow(resized_image, cmap='gray')
    plt.show()